{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrPKcHZX1UL3",
        "outputId": "12cf4247-624a-4700-e821-dcdfb364a2f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [50:03<00:00,  3.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Все данные успешно сохранены в один файл.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import requests\n",
        "import time\n",
        "import csv\n",
        "import datetime as dt\n",
        "import json\n",
        "import re\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.dates\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pandas_datareader as pdr\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from pathlib import Path\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from dateutil.parser import parse\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, date, time\n",
        "\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = None\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "os.makedirs('/content/drive/MyDrive/DenoiseLAB/Task_75_Moex/SmartLab_2022/Firm/', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/DenoiseLAB/Task_75_Moex/SmartLab_2023/Firm/', exist_ok=True)\n",
        "path_save_data_2022_firm = '/content/drive/MyDrive/DenoiseLAB/Task_75_Moex/SmartLab_2022/Firm'\n",
        "path_save_data_2023_firm = '/content/drive/MyDrive/DenoiseLAB/Task_75_Moex/SmartLab_2023/Firm'\n",
        "all_data = pd.DataFrame(columns=['date', 'company', 'text'])\n",
        "\n",
        "# получение_списка_уникальных_авторов_фирм_на_странице\n",
        "def unique_autors(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    unique_strings = set()\n",
        "    for tag_i in soup.find_all(\"i\"):\n",
        "        if tag_i.string:\n",
        "            unique_strings.add(tag_i.string.strip())\n",
        "    return unique_strings\n",
        "\n",
        "def unique_autors_firm(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    unique_strings_autor_firm = set()\n",
        "    div_tags = soup.find_all(\"div\", class_=\"cmt_body\")\n",
        "    unique_strings_autor_firm = [div_tag.find(\"a\", class_=\"a_thread\") for div_tag in div_tags]\n",
        "    for name in range(len(unique_strings_autor_firm)):\n",
        "        unique_strings_autor_firm[name] = str(unique_strings_autor_firm[name])\n",
        "        unique_strings_autor_firm[name] = unique_strings_autor_firm[name][:-4]\n",
        "        unique_strings_autor_firm[name] = unique_strings_autor_firm[name].split('>')[-1]\n",
        "\n",
        "    return unique_strings_autor_firm\n",
        "\n",
        "def unique_date_firm(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    unique_strings_date_firm = set()\n",
        "    div_tags = soup.find_all(\"div\", class_=\"cmt_body\")\n",
        "    unique_strings_date_firm = [div_tag.find(\"a\", class_=\"a_time\") for div_tag in div_tags]\n",
        "    for name in range(len(unique_strings_date_firm)):\n",
        "        unique_strings_date_firm[name] = str(unique_strings_date_firm[name])\n",
        "        unique_strings_date_firm[name] = unique_strings_date_firm[name][:-33]\n",
        "        unique_strings_date_firm[name] = unique_strings_date_firm[name].split('datetime=\"')[-1]\n",
        "\n",
        "    return unique_strings_date_firm\n",
        "\n",
        "def unique_autors_message_firm(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    unique_strings_message_firm = set()\n",
        "    div_tags = soup.find_all(\"div\", class_=\"text\")\n",
        "    unique_strings_message_firm = [div_tag.get_text() for div_tag in div_tags]\n",
        "    return unique_strings_message_firm\n",
        "\n",
        "def get_data_page_firm(start_page, end_page):\n",
        "    global all_data  # Используем глобальный DataFrame для накопления данных\n",
        "\n",
        "    for page_number in tqdm(range(start_page, end_page)):\n",
        "        url_page = f\"https://smart-lab.ru/forum/comments/page{page_number}/\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url_page)\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            date_col = unique_date_firm(url_page)\n",
        "            messages_col = unique_autors_message_firm(url_page)\n",
        "            company_col = unique_autors_firm(url_page)\n",
        "\n",
        "            # Находим самый короткий список\n",
        "            shortest_list = min(len(date_col), len(company_col), len(messages_col))\n",
        "            date_col = date_col[:shortest_list]\n",
        "            company_col = company_col[:shortest_list]\n",
        "            messages_col = messages_col[:shortest_list]\n",
        "\n",
        "            # Создаем DataFrame для текущей страницы\n",
        "            df = pd.DataFrame({'date': date_col, 'company': company_col,\n",
        "                             'text': messages_col})\n",
        "\n",
        "            # Добавляем данные в общий DataFrame\n",
        "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
        "\n",
        "            # Сохраняем промежуточные результаты каждые 100 страниц\n",
        "            if page_number % 100 == 0:\n",
        "                all_data.to_csv(f'{path_save_data_2022_firm}/all_comments.csv', index=False, encoding='utf-8')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обработке страницы {page_number}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "start_page = 17000\n",
        "end_page = 18000\n",
        "\n",
        "\n",
        "all_comments = get_data_page_firm(start_page, end_page)\n",
        "\n",
        "#в один файл\n",
        "all_comments.to_csv(f'{path_save_data_2022_firm}/all_comments_final.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Все данные успешно сохранены в один файл.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5dsXBQ0X1Uw8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}